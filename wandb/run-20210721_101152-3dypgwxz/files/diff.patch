diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index 27d478d..b2c49e2 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20210421_151007-16qry78i/logs/debug-internal.log
\ No newline at end of file
+run-20210721_101152-3dypgwxz/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index 890772d..fa45250 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20210421_151007-16qry78i/logs/debug.log
\ No newline at end of file
+run-20210721_101152-3dypgwxz/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index fde8811..5aafba6 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20210421_151007-16qry78i
\ No newline at end of file
+run-20210721_101152-3dypgwxz
\ No newline at end of file
diff --git a/weightsbiases.py b/weightsbiases.py
index 76e99c8..e92dfd2 100644
--- a/weightsbiases.py
+++ b/weightsbiases.py
@@ -1,20 +1,43 @@
+import time
+import gym
+from stable_baselines3 import PPO
+from stable_baselines3.common.monitor import Monitor
+from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder
+from wandb.integration.sb3 import WandbCallback
 import wandb
 
-import gym
-from stable_baselines3 import A2C
+config = {"policy_type": "MlpPolicy", "total_timesteps": 25000}
+experiment_name = f"PPO_{int(time.time())}"
+
+# Initialise a W&B run
+wandb.init(
+    name=experiment_name,
+    project="sb3",
+    config=config,
+    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics
+    monitor_gym=True,  # auto-upload the videos of agents playing the game
+    save_code=True,  # optional
+)
+
+def make_env():
+    env = gym.make("CartPole-v1")
+    env = Monitor(env)  # record stats such as returns
+    return env
 
-wandb.init(project='baseline-integration')
+env = DummyVecEnv([make_env])
 
-env = gym.make('CartPole-v1')
+env = VecVideoRecorder(env, "videos",
+    record_video_trigger=lambda x: x % 2000 == 0, video_length=200)
 
-model = A2C('MlpPolicy', env, verbose=1)
-model.learn(total_timesteps=10000)
+model = PPO(config["policy_type"], env, verbose=1,
+    tensorboard_log=f"runs/{experiment_name}")
 
-obs = env.reset()
-for i in range(1000):
-    action, _state = model.predict(obs, deterministic=True)
-    obs, reward, done, info = env.step(action)
-    wandb.log({'reward': reward})
-    #env.render()
-    if done:
-      obs = env.reset()
\ No newline at end of file
+# Add the WandbCallback 
+model.learn(
+    total_timesteps=config["total_timesteps"],
+    callback=WandbCallback(
+        gradient_save_freq=100,
+        model_save_freq=1000,
+        model_save_path=f"models/{experiment_name}",
+    ),
+)
\ No newline at end of file
